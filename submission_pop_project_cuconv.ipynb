{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs3kIIhdwGF2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAxtGHkzEGVm",
        "outputId": "48207051-0f13-4be7-b327-ead3360d17b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMqQiqpYEILl",
        "outputId": "a56b482b-bf6a-44f5-83f9-fcc64a4899b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmp1hf8pu65\".\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input is instantiated by all 1s\n",
        "\n",
        "kernel is instantiated by all 2s"
      ],
      "metadata": {
        "id": "cOp3OEmcyb5H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic_IqRE7wyNm",
        "outputId": "ac3f97a2-5996-458f-eeb6-04b8d2e3ef7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90 90 90 \n",
            "90 90 90 \n",
            "90 90 90 \n",
            "####\n",
            "Time measured: 0.000264 seconds.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%cuda\n",
        "//parallel code implementation of cuConv for n>=1 (simple without tiling)\n",
        "#include<bits/stdc++.h>\n",
        "#include <sys/time.h>\n",
        "\n",
        "#define MAX_DEPTH 1024\n",
        "#define THREADS 32\n",
        "#define debug 1\n",
        "#define N 1\n",
        "#define C 5\n",
        "#define H 5\n",
        "#define K 3\n",
        "\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "\n",
        "__global__ void stage1kernel(int* d_input, int* d_filter, int* d_partial, int cf, int hf, int wf, int w, int h, int r_field_size){\n",
        "      __shared__ int S[MAX_DEPTH]; //this is the shared memory\n",
        "      int tid = threadIdx.x + threadIdx.y*(w-wf+1);\n",
        "      if(tid < cf){ //maximum depth supported with this implementation is 1024\n",
        "          S[tid] = d_filter[tid*(hf*wf) + blockIdx.y*(wf) + blockIdx.x];\n",
        "\n",
        "\n",
        "      }\n",
        "      __syncthreads();\n",
        "      if(threadIdx.x<w-wf+1 && threadIdx.y < h-hf+1){\n",
        "        d_partial[ (blockIdx.y*(wf) + blockIdx.x)*r_field_size + threadIdx.y*(w-wf+1) + threadIdx.x] = 0;\n",
        "        for(int i=0;i<cf;i++){\n",
        "            int iy = (blockIdx.y + threadIdx.y);\n",
        "            int ix = (blockIdx.x + threadIdx.x);\n",
        "            int iz = i;\n",
        "            d_partial[ (blockIdx.y*(wf) + blockIdx.x)*r_field_size + threadIdx.y*(w-wf+1) + threadIdx.x] += d_input[ iz*w*h + iy*w + ix ]*S[i];\n",
        "            //          ^^this is the no. of rows done ^^space each took ^^loc in a particular matrix\n",
        "          }\n",
        "      }\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "__global__ void stage2kernel(int* d_partial, int n, int m, int depth){\n",
        "    int stride = 1;\n",
        "    int tid = threadIdx.x;\n",
        "    while(stride < depth){\n",
        "\n",
        "        if(tid%(stride*2)==0 && tid+stride < depth){\n",
        "\n",
        "            d_partial[tid*(n*m) + blockIdx.y*n + blockIdx.x] += d_partial[(tid+stride)*(n*m) + blockIdx.y*n + blockIdx.x];\n",
        "        }\n",
        "\n",
        "      stride = stride*2;\n",
        "        __syncthreads();\n",
        "\n",
        "    }\n",
        "\n",
        " }\n",
        "\n",
        "\n",
        "int main(){\n",
        "   struct timeval begin, end;\n",
        "   int *input;\n",
        "   int n=N,c=C,h=H,w=H; //only doing for n=1 now\n",
        "   int input_size = n*c*h*w;\n",
        "   input=(int*)malloc(input_size*sizeof(int));\n",
        "\n",
        "   int *filter;\n",
        "   int cf=C,hf=K,wf=K;\n",
        "   int filter_size = cf*hf*wf;\n",
        "   filter=(int*)malloc(filter_size*sizeof(int));\n",
        "\n",
        "\n",
        "   //initializing the 4D input\n",
        "   for(int i=0;i<n;i++){\n",
        "       for(int j=0;j<c;j++){\n",
        "           for(int k=0;k<h;k++){\n",
        "               for(int l=0;l<w;l++){\n",
        "                   input[ (c*h*w)*i + (h*w)*j + (w)*k + l ] = 1;\n",
        "               }\n",
        "           }\n",
        "       }\n",
        "   }\n",
        "\n",
        "   //initializing the 3D filter\n",
        "   for(int i=0;i<cf;i++){\n",
        "       for(int j=0;j<hf;j++){\n",
        "           for(int k=0;k<wf;k++){\n",
        "              filter[ (hf*wf)*i + (wf)*j + k ] = 2;\n",
        "           }\n",
        "       }\n",
        "   }\n",
        "   int *d_input, *d_filter;\n",
        "    int filter_rows = hf*wf;\n",
        "    int r_field_size = (w-wf+1)*(h-hf+1);\n",
        "\n",
        "    cudaMalloc(&d_input,c*h*w*sizeof(int));\n",
        "    cudaMalloc(&d_filter,filter_size*sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_filter, filter , filter_size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    int* d_partial;\n",
        "    cudaMalloc(&d_partial,filter_rows*r_field_size*sizeof(int));\n",
        "    dim3 grid(wf,hf);\n",
        "    dim3 block(THREADS, THREADS);\n",
        "    int* output;\n",
        "    output=(int*)malloc(n*r_field_size*sizeof(int));\n",
        "\n",
        "    gettimeofday(&begin, 0);\n",
        "   for(int i=0;i<n;i++){\n",
        "    cudaMemcpy(d_input, input +i*c*h*w, c*h*w* sizeof(int), cudaMemcpyHostToDevice);\n",
        "    stage1kernel<<<grid, block>>>(d_input, d_filter, d_partial, cf, hf, wf, w, h, r_field_size);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    dim3 grid2(w-wf+1,h-hf+1);\n",
        "    int threads2 = filter_rows;\n",
        "\n",
        "    stage2kernel<<<grid2,threads2 >>>(d_partial, w-wf+1, h-hf+1, filter_rows);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaMemcpy(output + i*(w-wf+1)*(h-hf+1), d_partial , r_field_size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    }\n",
        "     gettimeofday(&end, 0);\n",
        "\n",
        "\n",
        "  if(debug){\n",
        "   //display the convolution results\n",
        "   for(int l=0;l<n;l++){\n",
        "      for(int i=0;i<h-hf+1;i++){\n",
        "          for(int j=0;j<w-wf+1;j++){\n",
        "              cout<<output[l*(w-wf+1)*(h-hf+1) + (w-wf+1)*i + j]<<\" \";\n",
        "          }\n",
        "          cout<<\"\\n\";\n",
        "      }\n",
        "      cout<<\"####\\n\";\n",
        "  }\n",
        "}\n",
        "    long seconds = end.tv_sec - begin.tv_sec;\n",
        "    long microseconds = end.tv_usec - begin.tv_usec;\n",
        "    double elapsed = seconds + microseconds*1e-6;\n",
        "    printf(\"Time measured: %.6f seconds.\\n\", elapsed);\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please check output in C.txt"
      ],
      "metadata": {
        "id": "LnMfbJ0tyK7I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si3ftplakV1B",
        "outputId": "8386505d-64fe-4bae-f07e-fde8e743e257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%cuda\n",
        "//parallel code implementation of cuConv for n>=1, supporting bigger input with tiling implementation\n",
        "#include<bits/stdc++.h>\n",
        "#include <sys/time.h>\n",
        "\n",
        "#define MAX_DEPTH 1024\n",
        "#define THREADS 32\n",
        "#define debug 1\n",
        "#define N 1\n",
        "#define C 25\n",
        "#define H 30\n",
        "#define K 1\n",
        "#define TILE_WIDTH 32\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "\n",
        "__global__ void stage1kernel(int* d_input, int* d_filter, int* d_partial, int cf, int hf, int wf, int w, int h, int r_field_size){\n",
        "      __shared__ int S[MAX_DEPTH]; //this is the shared memory\n",
        "      int tid = threadIdx.x + threadIdx.y*TILE_WIDTH;\n",
        "      if(tid < cf){ //maximum depth supported with this implementation is 1024\n",
        "          S[tid] = d_filter[tid*(hf*wf) + blockIdx.y*(wf) + blockIdx.x];\n",
        "\n",
        "      }\n",
        "      __syncthreads();\n",
        "\n",
        "      for(int y=0;y<h-hf+1;y+=TILE_WIDTH){\n",
        "          for(int x=0;x<w-wf+1;x+=TILE_WIDTH){\n",
        "              int tx = x + threadIdx.x;\n",
        "              int ty = y + threadIdx.y;\n",
        "              if(tx<w-wf+1 && ty < h-hf+1){\n",
        "                d_partial[ (blockIdx.y*(wf) + blockIdx.x)*r_field_size + ty*(w-wf+1) + tx] = 0;\n",
        "                for(int i=0;i<cf;i++){\n",
        "                    int iy = (blockIdx.y + ty);\n",
        "                    int ix = (blockIdx.x + tx);\n",
        "                    int iz = i;\n",
        "\n",
        "\n",
        "                    d_partial[ (blockIdx.y*(wf) + blockIdx.x)*r_field_size + ty*(w-wf+1) + tx] += d_input[ iz*w*h + iy*w + ix ]*S[i];\n",
        "                    //          ^^this is the no. of rows done ^^space each took ^^loc in a particular matrix\n",
        "\n",
        "\n",
        "                  }\n",
        "              }\n",
        "\n",
        "\n",
        "\n",
        "          }\n",
        "\n",
        "      }\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "__global__ void stage2kernel(int* d_partial, int n, int m, int depth){\n",
        "\n",
        "\n",
        "    for(int x=0;x*TILE_WIDTH<n;x++){\n",
        "        for(int y=0;y*TILE_WIDTH<m;y++){\n",
        "            int bx = x*TILE_WIDTH + blockIdx.x, by = y*TILE_WIDTH + blockIdx.y;\n",
        "            if(bx<n && by < m){\n",
        "\n",
        "                int stride = 1;\n",
        "                int tid = threadIdx.x;\n",
        "                while(stride < depth){\n",
        "\n",
        "\n",
        "                    if(tid%(stride*2)==0 && tid+stride < depth){\n",
        "\n",
        "                        d_partial[tid*(n*m) + by*n + bx] += d_partial[(tid+stride)*(n*m) + by*n + bx];\n",
        "                    }\n",
        "                  stride = stride*2;\n",
        "                    __syncthreads();\n",
        "\n",
        "\n",
        "\n",
        "                }\n",
        "            }\n",
        "\n",
        "        }\n",
        "    }\n",
        "\n",
        " }\n",
        "\n",
        "\n",
        "int main(){\n",
        "    freopen(\"C.txt\", \"w\", stdout); //for writing to C.txt using cout\n",
        "\n",
        "   struct timeval begin, end;\n",
        "   int *input;\n",
        "   int n=N,c=C,h=H,w=H; //only doing for n=1 now\n",
        "   int input_size = n*c*h*w;\n",
        "   input=(int*)malloc(input_size*sizeof(int));\n",
        "\n",
        "   int *filter;\n",
        "   int cf=C,hf=K,wf=K;\n",
        "   int filter_size = cf*hf*wf;\n",
        "   filter=(int*)malloc(filter_size*sizeof(int));\n",
        "\n",
        "\n",
        "   //initializing the 4D input\n",
        "   for(int i=0;i<n;i++){\n",
        "       for(int j=0;j<c;j++){\n",
        "           for(int k=0;k<h;k++){\n",
        "               for(int l=0;l<w;l++){\n",
        "                   input[ (c*h*w)*i + (h*w)*j + (w)*k + l ] = 1;\n",
        "               }\n",
        "           }\n",
        "       }\n",
        "   }\n",
        "\n",
        "   //initializing the 3D filter\n",
        "   for(int i=0;i<cf;i++){\n",
        "       for(int j=0;j<hf;j++){\n",
        "           for(int k=0;k<wf;k++){\n",
        "              filter[ (hf*wf)*i + (wf)*j + k ] = 2;\n",
        "           }\n",
        "       }\n",
        "   }\n",
        "   int *d_input, *d_filter;\n",
        "    int filter_rows = hf*wf;\n",
        "    int r_field_size = (w-wf+1)*(h-hf+1);\n",
        "\n",
        "    cudaMalloc(&d_input,c*h*w*sizeof(int));\n",
        "    cudaMalloc(&d_filter,filter_size*sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_filter, filter , filter_size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    dim3 grid(wf,hf);\n",
        "    dim3 block(TILE_WIDTH, TILE_WIDTH);\n",
        "    int* output;\n",
        "    output=(int*)malloc(n*r_field_size*sizeof(int));\n",
        "\n",
        "    gettimeofday(&begin, 0);\n",
        "   for(int i=0;i<n;i++){\n",
        "    int* d_partial;\n",
        "    cudaMalloc(&d_partial,filter_rows*r_field_size*sizeof(int));\n",
        "    cudaMemcpy(d_input, input +i*c*h*w, c*h*w* sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    stage1kernel<<<grid, block>>>(d_input, d_filter, d_partial, cf, hf, wf, w, h, r_field_size);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    dim3 grid2(TILE_WIDTH,TILE_WIDTH);\n",
        "    int threads2 = filter_rows;\n",
        "    stage2kernel<<<grid2,threads2 >>>(d_partial, w-wf+1, h-hf+1, filter_rows);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaMemcpy(output + i*(w-wf+1)*(h-hf+1), d_partial , r_field_size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    }\n",
        "     gettimeofday(&end, 0);\n",
        "\n",
        "\n",
        "  // check for error\n",
        "  cudaError_t error = cudaGetLastError();\n",
        "  if(error != cudaSuccess)\n",
        "  {\n",
        "    // print the CUDA error message and exit\n",
        "    printf(\"CUDA error: %s\\n\", cudaGetErrorString(error));\n",
        "    exit(-1);\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "  if(debug){\n",
        "   //display the convolution results\n",
        "   for(int l=0;l<n;l++){\n",
        "      for(int i=0;i<h-hf+1;i++){\n",
        "          for(int j=0;j<w-wf+1;j++){\n",
        "              cout<<output[l*(w-wf+1)*(h-hf+1) + (w-wf+1)*i + j]<<\" \";\n",
        "          }\n",
        "          cout<<\"\\n\";\n",
        "      }\n",
        "      cout<<\"####\\n\";\n",
        "  }\n",
        "}\n",
        "    long seconds = end.tv_sec - begin.tv_sec;\n",
        "    long microseconds = end.tv_usec - begin.tv_usec;\n",
        "    double elapsed = seconds + microseconds*1e-6;\n",
        "    printf(\"Time measured: %.6f seconds.\\n\", elapsed);\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vj-yK5ZrjMLa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}